{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import euler_gpu\n",
    "import os, random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from math import pi as PI\n",
    "device = torch.device(\"cuda:0\")\n",
    "BATCH_SIZE = 256\n",
    "red_channel = 1\n",
    "fixed_frame = 0\n",
    "\n",
    "MAX_NUM_CENTROIDS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_path = \"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250214153740_RFa/20250214_Experiment_01_0-1999.tif\"\n",
    "inputs = tifffile.imread(tiff_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Video by adjacent frames over multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH_CONST = np.array([0])\n",
    "XY_RANGE_STEP2 = np.concatenate((np.linspace(-0.005, 0.005, 10, dtype=np.float32), TH_CONST)) # Just using this here so we don't have ot redefine\n",
    "ALIGN_TH_RANGE = np.concatenate((np.linspace(0, .5, 10, dtype=np.float32), np.linspace(359.5, 360, 10, dtype=np.float32), TH_CONST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_folder = \"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250214153740_RFa\"\n",
    "files = [\"20250214_Experiment_01_0-1999.tif\", \"20250214_Experiment_01_2000-3999.tif\", \"20250214_Experiment_01_4000-5999.tif\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def segment_sparse_blobs(\n",
    "    image,\n",
    "    blur_sigma=1.5,\n",
    "    threshold_method=\"adaptive\",  # \"adaptive\" or \"global\"\n",
    "    global_thresh_val=100,\n",
    "    min_area=20,\n",
    "    morph_kernel_size=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Segment sparse blobs in a grayscale image.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Grayscale image.\n",
    "        blur_sigma (float): Gaussian blur strength.\n",
    "        threshold_method (str): \"adaptive\" or \"global\".\n",
    "        global_thresh_val (int): Threshold value if using global.\n",
    "        min_area (int): Minimum area (pixels) to keep a blob.\n",
    "        morph_kernel_size (int): Size for closing small holes.\n",
    "\n",
    "    Returns:\n",
    "        labeled_mask (np.ndarray): Connected component labels.\n",
    "        centroids (np.ndarray): (x, y) centroid coordinates of blobs.\n",
    "        contours (list): List of contour arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to 8-bit if needed\n",
    "    if image.dtype != np.uint8:\n",
    "        image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Step 1: Smooth image\n",
    "    blurred = cv2.GaussianBlur(image, (0, 0), blur_sigma) # Pretty sure this doesn't do anything, but it works so I'm not going to mess with it\n",
    "\n",
    "    # Step 2: Threshold\n",
    "    if threshold_method == \"adaptive\":\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY, blockSize=35, C=-10)\n",
    "    else:\n",
    "        _, binary = cv2.threshold(blurred, global_thresh_val, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Step 3: Morphological cleanup\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (morph_kernel_size,) * 2)\n",
    "    cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Step 4: Label connected components\n",
    "    labeled, num_labels = label(cleaned)\n",
    "\n",
    "    # Step 5: Filter small blobs and compute centroids\n",
    "    centroids = np.zeros((num_labels, 2), dtype=np.float32)\n",
    "    contours = []\n",
    "    output_mask = np.zeros_like(labeled)\n",
    "\n",
    "    for label_val in range(1, num_labels + 1):\n",
    "        mask = (labeled == label_val).astype(np.uint8)\n",
    "        area = np.sum(mask)\n",
    "\n",
    "        if area >= min_area:\n",
    "            output_mask[mask > 0] = label_val\n",
    "            M = cv2.moments(mask)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                # centroids.append((cx, cy))\n",
    "                centroids[label_val - 1] = (cx, cy)\n",
    "            cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contours.extend(cnts)\n",
    "\n",
    "    return output_mask, centroids, contours\n",
    "\n",
    "def pad_centroids(centroids): # Not entirely necessary, but I like being able to catch these centroid overflows\n",
    "    \"\"\"\n",
    "    Pad the centroids array to the globally specificed maximum number of centroids.\n",
    "\n",
    "    Args:\n",
    "        centroids (np.ndarray): Array of centroids.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Padded centroids array.\n",
    "    \"\"\"\n",
    "\n",
    "    if centroids.shape[0] > MAX_NUM_CENTROIDS: # Handle too many centroids by randomly sampling\n",
    "        print(f\"Warning: Too many centroids ({centroids.shape[0]}). Truncated after {MAX_NUM_CENTROIDS} centroids. This is NOT a truly random set.\")\n",
    "        # return centroids[np.random.choice(centroids.shape[0], MAX_NUM_CENTROIDS, replace=False)] # Was going to sample but this allows us entry consistency\n",
    "        return centroids[:MAX_NUM_CENTROIDS]\n",
    "    padded_centroids = np.full((MAX_NUM_CENTROIDS, 2), -1, dtype=np.float32)\n",
    "    num_centroids = centroids.shape[0]\n",
    "    padded_centroids[:num_centroids] = centroids\n",
    "    return padded_centroids\n",
    "\n",
    "\n",
    "def get_cents_from_rois(roi_img):\n",
    "    centroids = np.zeros((MAX_NUM_CENTROIDS, 2), dtype=np.float32)\n",
    "\n",
    "    for label_val in range(1, np.max(roi_img) + 1):\n",
    "        mask = (roi_img == label_val).astype(np.uint8)\n",
    "\n",
    "        M = cv2.moments(mask)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "            # centroids.append((cx, cy))\n",
    "            centroids[label_val - 1] = (cx, cy)\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import median, gaussian\n",
    "from skimage.morphology import disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [43:30<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0089], device='cuda:0'), tensor([0.0222], device='cuda:0'), tensor([6.6283e-05], device='cuda:0'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/PB/lib/python3.11/site-packages/tifffile/tifffile.py:3784: UserWarning: <tifffile.TiffWriter 'RIG_20250214_Ex…nt_01_0-1999.tif'> truncating ImageJ file\n",
      "  warnings.warn(\n",
      "100%|██████████| 2000/2000 [43:23<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0161], device='cuda:0'), tensor([-0.0133], device='cuda:0'), tensor([0.0060], device='cuda:0'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/PB/lib/python3.11/site-packages/tifffile/tifffile.py:3784: UserWarning: <tifffile.TiffWriter 'RIG_20250214_Exp…1_2000-3999.tif'> truncating ImageJ file\n",
      "  warnings.warn(\n",
      "100%|██████████| 2000/2000 [45:31<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-9.3132e-10], device='cuda:0'), tensor([-0.0056], device='cuda:0'), tensor([0.0090], device='cuda:0'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/PB/lib/python3.11/site-packages/tifffile/tifffile.py:3784: UserWarning: <tifffile.TiffWriter 'RIG_20250214_Exp…1_4000-5999.tif'> truncating ImageJ file\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Align Video by adjacent frames (i.e. look for small variations of the previous transformation of the image that align would align it best with the fixed image)\n",
    "other_chan = (red_channel - 1) % inputs.shape[1]\n",
    "\n",
    "fixed_image = inputs[0, red_channel]\n",
    "dtyp = fixed_image.dtype\n",
    "fixed_image = (segment_sparse_blobs(fixed_image)[0] > 0).astype(dtyp)\n",
    "\n",
    "pre_centroids = np.full((inputs.shape[0], MAX_NUM_CENTROIDS, 2), fill_value = -1, dtype=np.float32)\n",
    "rig_centroids = np.full((inputs.shape[0], MAX_NUM_CENTROIDS, 2), fill_value = -1, dtype=np.float32)\n",
    "\n",
    "prev_transform = (torch.tensor([0.0], device='cuda:0'), torch.tensor([0.0], device='cuda:0'), torch.tensor([0.0], device='cuda:0'))\n",
    "\n",
    "for file in files:\n",
    "    if file == files[0]: # Just saves some time loading large files.\n",
    "        ins = inputs\n",
    "    else:\n",
    "        ins = tifffile.imread(os.path.join(tiff_folder, file))\n",
    "\n",
    "    out = np.empty_like(ins)\n",
    "    for frame in tqdm(range(ins.shape[0])):\n",
    "        moving_image = ins[frame, red_channel]\n",
    "\n",
    "        ## Image Modifications before Alignment (make sure match fixed image)\n",
    "        rois, centroids, _ = segment_sparse_blobs(moving_image)\n",
    "        moving_image = (rois > 0).astype(dtyp)\n",
    "\n",
    "        pre_centroids[frame] = pad_centroids(centroids)\n",
    "\n",
    "        align_x_range_2 = np.add(XY_RANGE_STEP2, prev_transform[0].cpu().numpy())\n",
    "        align_y_range_2 = np.add(XY_RANGE_STEP2, prev_transform[1].cpu().numpy())\n",
    "        align_the_range = np.mod(np.add(ALIGN_TH_RANGE, (prev_transform[2].cpu().numpy() * 180) / PI), 360) # For some reason the search is in degrees but the warp is in radians\n",
    "        memory_dict = euler_gpu.initialize(fixed_image, moving_image, align_x_range_2, align_y_range_2, align_the_range, BATCH_SIZE, device)\n",
    "        best_score, best_transformation = euler_gpu.grid_search(memory_dict)\n",
    "        \n",
    "        in_img = ins[frame, red_channel]\n",
    "        in_img = torch.Tensor(in_img[np.newaxis, np.newaxis, ...]).to(device=device)\n",
    "        out[frame, red_channel] = euler_gpu.transform_image(in_img, best_transformation[0], best_transformation[1], best_transformation[2], memory_dict).cpu().numpy()\n",
    "        in_img = ins[frame, other_chan]\n",
    "        in_img = torch.Tensor(in_img[np.newaxis, np.newaxis, ...]).to(device=device)\n",
    "        out[frame, other_chan] = euler_gpu.transform_image(in_img, best_transformation[0], best_transformation[1], best_transformation[2], memory_dict).cpu().numpy()\n",
    "        \n",
    "\n",
    "        ## Noting this here because I can't think of a better place: centroid entry consistency is only guaranteed before and after the transformation, not between frames\n",
    "            # e.g. the first entry in pre_ will be the same centroid as in the rig_ but not if you increase the index by one\n",
    "        rig_centroids[frame] = pad_centroids(get_cents_from_rois(\n",
    "            euler_gpu.transform_image(\n",
    "                torch.tensor(rois[np.newaxis, np.newaxis, ...], device=device, dtype=in_img.dtype), \n",
    "                best_transformation[0], best_transformation[1], best_transformation[2], memory_dict, interpolation=\"nearest\").cpu().numpy().astype(int)[0, 0]))\n",
    "        \n",
    "        if file == files[0] and frame == 0:\n",
    "            # print(best_transformation) # This should be all zeros\n",
    "            assert np.all((pre_centroids[frame] < 0) | (rig_centroids[frame] < 0) | (rig_centroids[frame] == pre_centroids[frame])), \"Centroids should not warp on the first frame\"\n",
    "\n",
    "        prev_transform = best_transformation # Restart the next loop searching around the last best transformation\n",
    "\n",
    "    print(prev_transform) # Printing so you can restore if something crashes\n",
    "    tifffile.imwrite(os.path.join(tiff_folder, \"RIG_\" + file), out, imagej=True)\n",
    "    # break\n",
    "\n",
    "    with h5py.File(os.path.join(tiff_folder, \"RIG_CENTS_\" + file.replace(\"tiff\", \"h5\").replace(\"tif\", \"h5\")), 'w') as f:\n",
    "        f.create_dataset(\"raw_img_centroids\", data=pre_centroids)\n",
    "        f.create_dataset(\"post_rig_centroids\", data=rig_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(tiff_folder, \"RIG_CENTS_\" + file.replace(\"tiff\", \"h5\").replace(\"tif\", \"h5\")), 'w') as f:\n",
    "    f.create_dataset(\"raw_img_centroids\", data=pre_centroids)\n",
    "    f.create_dataset(\"post_rig_centroids\", data=rig_centroids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
