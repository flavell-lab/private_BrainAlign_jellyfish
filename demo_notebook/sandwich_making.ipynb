{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py, tifffile\n",
    "import numpy as np\n",
    "import random, cv2\n",
    "\n",
    "os.chdir(\"../scripts/data_preparation\")  \n",
    "label_shape = (100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"/home/brian/data4/brian/PBnJ/jelly_h5s/RFamide/mixed_datasets\"\n",
    "\n",
    "raw = True # If true, we will make the h5s from raw tif files provided\n",
    "frames_per_dataset = 100 # How many frames from each dataset we should sample\n",
    "\n",
    "add_extra = False # Should we add more frames from an nrrd to augment the labeled frames\n",
    "num_frames_to_add = 500 # If we add extra frames, how many should we add\n",
    "padding = \"single\" # \"single\" or \"double\" - Do you want to pad on both the top and bottom or just the bottom\n",
    "side_len = 1024 # What the cropped image size should be (enforcing square) should prob be 1024, 1080, or 1200\n",
    "\n",
    "augmentation = None # Options are \"log\" \"sqrt\" None\n",
    "\n",
    "assert padding in [\"single\", \"double\"], f\"Padding must be 'single' or 'double', not {padding}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the dataset from scratch if needed\n",
    "\n",
    "Assumes you want all moving to one fixed frame\n",
    "\n",
    "TODO: Reconsider this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Folder_20250219110008_RFa_swim...\n",
      "Saved 100 frames for Folder_20250219110008_RFa_swim\n",
      "Processing Folder_20250219120831_RFa...\n",
      "Saved 100 frames for Folder_20250219120831_RFa\n",
      "Processing Folder_20250214153740_RFa...\n",
      "Saved 100 frames for Folder_20250214153740_RFa\n"
     ]
    }
   ],
   "source": [
    "if raw:\n",
    "    assert frames_per_dataset\n",
    "    n_frames = 2000 # How many frames are in each tif file\n",
    "    channel = 1 # Which channel to use (1 in RFa, 0 in normal - check)\n",
    "    input_files = {\"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250219110008_RFa_swim\" : [\n",
    "                                    \"20250219_Experiment_01_0-1999.tif\",\n",
    "                                    \"20250219_Experiment_01_2000-3999.tif\",\n",
    "                                    \"20250219_Experiment_01_4000-5999.tif\",\n",
    "                                    \"20250219_Experiment_01_8000-9999.tif\",\n",
    "                                    \"20250219_Experiment_01_6000-7999.tif\"],\n",
    "                    \"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250219120831_RFa\": [\n",
    "                                    \"20250219_Experiment_01_0-1999.tif\",\n",
    "                                    \"20250219_Experiment_01_2000-3999.tif\",\n",
    "                                    \"20250219_Experiment_01_4000-5999.tif\",\n",
    "                                    \"20250219_Experiment_01_6000-7999.tif\",\n",
    "                                    \"20250219_Experiment_01_8000-9999.tif\"],\n",
    "                    \"/storage/fs/store1/brian/swimming_videos_RFa/Folder_20250214153740_RFa\": [\n",
    "                                    \"20250214_Experiment_01_0-1999.tif\",\n",
    "                                    \"20250214_Experiment_01_2000-3999.tif\",\n",
    "                                    \"20250214_Experiment_01_4000-5999.tif\"],\n",
    "                    }\n",
    "\n",
    "    unlabs = np.full((30, 2), -1)\n",
    "\n",
    "    with h5py.File(os.path.join(base_folder, \"moving_images.h5\"), 'w-') as h5m, h5py.File(os.path.join(base_folder, \"fixed_images.h5\"), 'w-') as h5f, h5py.File(\n",
    "            os.path.join(base_folder, \"moving_labels.h5\"), 'w-') as h5ml, h5py.File(os.path.join(base_folder, \"fixed_labels.h5\"), 'w-') as h5fl:\n",
    "        dataset_ind = 0\n",
    "        for dataset_name, file_list in input_files.items():\n",
    "            file_list = [os.path.join(dataset_name, f) for f in file_list]\n",
    "\n",
    "            dataset_name = os.path.basename(dataset_name)\n",
    "\n",
    "            print(f\"Processing {dataset_name}...\")\n",
    "            file_list = list(file_list)\n",
    "            random.shuffle(file_list)\n",
    "\n",
    "            total_frames = len(file_list) * n_frames\n",
    "            if frames_per_dataset > total_frames:\n",
    "                raise ValueError(f\"Requested {frames_per_dataset} frames, but only {total_frames} available in {dataset_name}.\")\n",
    "\n",
    "            # Sample global frame indices\n",
    "            selected_global_indices = sorted(random.sample(range(total_frames), frames_per_dataset))\n",
    "            with open(os.path.join(base_folder, \"frame_log.txt\"), 'a') as f: # Save a log of the indices\n",
    "                f.write(f\"# {dataset_name}\\n\")\n",
    "                for idx in selected_global_indices:\n",
    "                    f.write(f\"{idx}\\n\")\n",
    "\n",
    "            # Iterate and extract frames\n",
    "            current_global_index = 0\n",
    "            saved_count = 0\n",
    "            for tif_path in file_list:\n",
    "                start, end = map(int, tif_path.split(\"_\")[-1].replace(\".tif\", \"\").split(\"-\"))\n",
    "\n",
    "                with tifffile.TiffFile(tif_path) as tif:\n",
    "                    arr = tif.asarray()\n",
    "                    if arr.ndim != 4:\n",
    "                        raise ValueError(f\"Expected shape (T, C, H, W) but got {arr.shape} in {tif_path}\")\n",
    "                    \n",
    "                    if start == 0:\n",
    "                        # Save the fixed image\n",
    "                        h5f.create_dataset(f\"{dataset_ind}_0to{dataset_ind}_0\", data=arr[0, channel])\n",
    "                        h5fl.create_dataset(f\"{dataset_ind}_0to{dataset_ind}_0\", data=unlabs)\n",
    "\n",
    "                    for idx in selected_global_indices:\n",
    "                        if idx >= start and idx <= end:\n",
    "                            local_idx = idx - start\n",
    "                            frame = arr[local_idx, channel]  # shape (H, W)\n",
    "                            ds_name = f\"{dataset_ind}_{idx}to{dataset_ind}_0\" # We're going to add the first number to diferentiate the datasets\n",
    "                            h5m.create_dataset(ds_name, data=frame)\n",
    "                            h5ml.create_dataset(ds_name, data=unlabs)\n",
    "\n",
    "                            saved_count += 1\n",
    "\n",
    "            dataset_ind += 1\n",
    "            print(f\"Saved {saved_count} frames for {dataset_name}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone fixed image to train\n",
    "Start with images (formatted as just 1 fixed image from Weissbourd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_images.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_images.h5\")\n",
    "moving_h5 = os.path.join(base_folder, \"moving_images.h5\")\n",
    "with h5py.File(old_fixed_h5, 'r') as f, h5py.File(moving_h5, 'r') as g, h5py.File(new_fixed_h5, 'w-') as fo:\n",
    "    if len(f.keys()) == 1:\n",
    "        img = f[list(f.keys())[0]][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = img)\n",
    "    else:\n",
    "        assert len(set([f.split(\"_\")[-1] for f in f.keys()])) == 1, \"Expected a file with either one image or one image per dataset\"\n",
    "        base_imgs = {}\n",
    "        for im in f.keys():\n",
    "            base_imgs[im.split(\"_\")[0]] = f[im][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = base_imgs[prob.split(\"_\")[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do the same for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_labels.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_labels.h5\")\n",
    "moving_h5 = os.path.join(base_folder, \"moving_labels.h5\")\n",
    "\n",
    "with h5py.File(old_fixed_h5, 'r') as f, h5py.File(moving_h5, 'r') as g, h5py.File(new_fixed_h5, 'w-') as fo:\n",
    "    if len(f.keys()) == 1:\n",
    "        labs = f[list(f.keys())[0]][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = labs)\n",
    "    else:\n",
    "        assert len(set([f.split(\"_\")[-1] for f in f.keys()])) == 1, \"Expected a file with either one image or one image per dataset\"\n",
    "        base_labs = {}\n",
    "        for im in f.keys():\n",
    "            base_labs[im.split(\"_\")[0]] = f[im][:]\n",
    "        for prob in g.keys():\n",
    "            fo.create_dataset(prob, data = base_labs[prob.split(\"_\")[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add a third column so the labels have three dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_labels.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_fixed_labels.h5\")\n",
    "with h5py.File(old_fixed_h5, 'r') as f:\n",
    "    assert f[list(f.keys())[0]][:].shape[1] == 2, \"This is meant to expand the labels from 2D to 3D\"\n",
    "    with h5py.File(new_fixed_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            labs = f[prob][:]\n",
    "            labs = np.concatenate((labs, np.zeros((labs.shape[0], 1))), 1)\n",
    "            labs = np.pad(labs, [(0,label_shape[0] - labs.shape[0]), (0,0)], \"constant\", constant_values=-1)\n",
    "            g.create_dataset(prob, data = labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_moving_h5 = os.path.join(base_folder, \"moving_labels.h5\")\n",
    "new_moving_h5 = os.path.join(base_folder, \"fixed_moving_labels.h5\")\n",
    "with h5py.File(old_moving_h5, 'r') as f:\n",
    "    assert f[list(f.keys())[0]][:].shape[1] == 2, \"This is meant to expand the labels from 2D to 3D\"\n",
    "    with h5py.File(new_moving_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            labs = f[prob][:]\n",
    "            labs = np.concatenate((labs, np.zeros((labs.shape[0], 1))), 1)\n",
    "            labs = np.pad(labs, [(0,label_shape[0] - labs.shape[0]), (0,0)], \"constant\", constant_values=-1)\n",
    "            g.create_dataset(prob, data = labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add a third dim for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_images.h5\")\n",
    "new_fixed_h5 = os.path.join(base_folder, \"fixed_fixed_fixed_images.h5\")\n",
    "with h5py.File(old_fixed_h5, 'r') as f:\n",
    "    assert len(f[list(f.keys())[0]][:].shape) == 2, \"This is meant to expand the images from 2D to 3D\"\n",
    "    with h5py.File(new_fixed_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            img = f[prob][:]\n",
    "            img = np.expand_dims(img, 2)\n",
    "            g.create_dataset(prob, data = img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_moving_h5 = os.path.join(base_folder, \"moving_images.h5\")\n",
    "new_moving_h5 = os.path.join(base_folder, \"fixed_moving_images.h5\")\n",
    "with h5py.File(old_moving_h5, 'r') as f:\n",
    "    assert len(f[list(f.keys())[0]][:].shape) == 2, \"This is meant to expand the images from 2D to 3D\"\n",
    "    with h5py.File(new_moving_h5, 'w-') as g:\n",
    "        for prob in f.keys():\n",
    "            img = f[prob][:]\n",
    "            img = np.expand_dims(img, 2)\n",
    "            g.create_dataset(prob, data = img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take unlabeled frames and add to a labeled h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_extra:\n",
    "\n",
    "    old_img_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/fixed_images.h5\"\n",
    "    new_img_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/fixed_images.h5\"\n",
    "\n",
    "    old_label_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/fixed_labels.h5\"\n",
    "    new_label_fixed_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/fixed_labels.h5\"\n",
    "\n",
    "    old_img_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/moving_images.h5\"\n",
    "    new_img_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/moving_images.h5\"\n",
    "\n",
    "    old_label_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_h5s/full_lab_movds/moving_labels.h5\"\n",
    "    new_label_moving_h5 = \"/home/brian/data4/brian/PBnJ/jelly_processed_data/mixed_lab_h5/moving_labels.h5\"\n",
    "\n",
    "    frame_folder = \"/home/brian/data4/brian/PBnJ/jelly_centroid_prep/zoomed_in_vid\"\n",
    "\n",
    "\n",
    "    frame_names = [\n",
    "        os.path.splitext(p)[0] for p in os.listdir(frame_folder)\n",
    "        if os.path.splitext(p)[-1] in [\".jpg\"]\n",
    "    ]\n",
    "\n",
    "    probs = []\n",
    "\n",
    "    for i in range(num_frames_to_add):\n",
    "        probs.append(tuple(random.sample(frame_names, 2)))\n",
    "\n",
    "    # old_mean = 0\n",
    "    old_max = 0\n",
    "    # num = 0\n",
    "\n",
    "    with h5py.File(new_img_fixed_h5, 'w-') as nif:\n",
    "        with h5py.File(new_label_fixed_h5, 'w-') as nlf:\n",
    "            with h5py.File(old_img_fixed_h5, 'r') as oif:\n",
    "                with h5py.File(old_label_fixed_h5, 'r') as olf:\n",
    "                    for prob in oif.keys():\n",
    "                        img = oif[prob][:]\n",
    "                        labs = olf[prob][:]\n",
    "\n",
    "                        # old_mean += np.mean(img)\n",
    "                        old_max = max(old_max, np.max(img))\n",
    "                        # num += 1\n",
    "\n",
    "                        nif.create_dataset(prob, data = img, dtype=float)\n",
    "                        nlf.create_dataset(prob, data = labs, dtype=np.float32)\n",
    "            \n",
    "            # old_mean = old_mean / num\n",
    "            scale_factor = old_max / 255 # Photo max val\n",
    "            \n",
    "            unlabeled_labs = np.ones_like(labs, dtype=np.float32) * -1\n",
    "            for fixed, moving in probs:\n",
    "                prob = f\"{moving}to{fixed}\"\n",
    "                img = cv2.imread(os.path.join(frame_folder, fixed + \".jpg\"))[:,:,0:1]\n",
    "                nif.create_dataset(prob, data = img * scale_factor, dtype=float)\n",
    "                nlf.create_dataset(prob, data = unlabeled_labs, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(new_img_moving_h5, 'w-') as nim:\n",
    "        with h5py.File(new_label_moving_h5, 'w-') as nlm:\n",
    "            with h5py.File(old_img_moving_h5, 'r') as oim:\n",
    "                with h5py.File(old_label_moving_h5, 'r') as olm:\n",
    "                    for prob in oim.keys():\n",
    "                        img = oim[prob][:]\n",
    "                        labs = olm[prob][:]\n",
    "                        nim.create_dataset(prob, data = img, dtype=float)\n",
    "                        nlm.create_dataset(prob, data = labs, dtype=np.float32)\n",
    "\n",
    "            for fixed, moving in probs:\n",
    "                prob = f\"{moving}to{fixed}\"\n",
    "                img = cv2.imread(os.path.join(frame_folder, moving + \".jpg\"))[:,:,0:1]\n",
    "                nim.create_dataset(prob, data = img * scale_factor, dtype=float)\n",
    "                nlm.create_dataset(prob, data = unlabeled_labs, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = base_folder\n",
    "out_dir = os.path.join(base_folder, \"cropped\")\n",
    "\n",
    "crop_shape = np.array((side_len, side_len, 1))\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with h5py.File(os.path.join(in_dir, \"fixed_moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"fixed_moving_labels.h5\"), 'r') as lmv, h5py.File(\n",
    "    os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "    with h5py.File(os.path.join(in_dir, \"fixed_fixed_fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_fixed_fixed_labels.h5\"), 'r') as lfx, h5py.File(\n",
    "        os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "        for prob in imv.keys():\n",
    "            img = imv[prob][:]\n",
    "            imgF = ifx[prob][:]\n",
    "\n",
    "            crop_offset = (img.shape - crop_shape) / 2\n",
    "            assert np.all(crop_offset == crop_offset.astype(int))\n",
    "            crop_offset = crop_offset.astype(int)\n",
    "\n",
    "            ## Crop the images\n",
    "            img = img[crop_offset[0]:crop_offset[0] + crop_shape[0], crop_offset[1]:crop_offset[1] + crop_shape[1], crop_offset[2]:crop_offset[2] + crop_shape[2]]\n",
    "            assert np.all(img.shape == crop_shape)\n",
    "\n",
    "            imgF = imgF[crop_offset[0]:crop_offset[0] + crop_shape[0], crop_offset[1]:crop_offset[1] + crop_shape[1], crop_offset[2]:crop_offset[2] + crop_shape[2]]\n",
    "            assert np.all(imgF.shape == crop_shape), imgF.shape\n",
    "\n",
    "            ## Adjust the labels\n",
    "            labs = lmv[prob][:]\n",
    "            neg_ones = labs < 0\n",
    "            labs = labs - crop_offset \n",
    "\n",
    "            labsF = lfx[prob][:]\n",
    "            neg_onesF = labsF < 0\n",
    "            labsF = labsF - crop_offset\n",
    "            assert np.all(neg_ones == neg_onesF), \"Fixed and Moving labels have a different number of non negative 1 labels\"\n",
    "            \n",
    "            # Remove centroids that are cropped out of the image\n",
    "            crop = np.logical_or(labs < 0, labs >= crop_shape)\n",
    "            crop = np.max(crop, axis=-1)\n",
    "            cropF = np.logical_or(labsF < 0, labsF >= crop_shape)\n",
    "            cropF = np.max(cropF, axis=-1)\n",
    "            crop = np.logical_or(crop, cropF) # If either the fixed or moving are out of bounds then exclude both\n",
    "\n",
    "            labs[crop] = -1\n",
    "            labsF[crop] = -1\n",
    "\n",
    "            labs[neg_ones] = -1 # Retain -1s\n",
    "            labsF[neg_ones] = -1 # Retain -1s\n",
    "\n",
    "            # Double check that all of the out of frame centroids are gone\n",
    "            assert np.all(np.logical_or(labs >= 0, labs == -1)), \"The crop results in moving centroids that are under bounds\"\n",
    "            assert np.all(labs < crop_shape[0]), \"The crop results in moving centroids that are over bounds\"\n",
    "            assert np.all(np.logical_or(labsF >= 0, labsF == -1)), \"The crop results in fixed centroids that are under bounds\"\n",
    "            assert np.all(labsF < crop_shape[0]), \"The crop results in fixed centroids that are over bounds\"\n",
    "\n",
    "            assert np.all((labs == -1) == (labsF == -1)), \"The labels that are excluded are not the same between moving and fixed\"\n",
    "\n",
    "\n",
    "            oimv.create_dataset(prob, data = img)\n",
    "            olmv.create_dataset(prob, data = labs)\n",
    "            oifx.create_dataset(prob, data = imgF)\n",
    "            olfx.create_dataset(prob, data = labsF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = os.path.join(base_folder, \"cropped\")\n",
    "out_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "if padding == \"single\":\n",
    "    padding = np.array([[0,0],[0,0],[1,0]])\n",
    "else:\n",
    "    padding = np.array([[0,0],[0,0],[1,1]])\n",
    "\n",
    "with h5py.File(os.path.join(in_dir, \"moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"moving_labels.h5\"), 'r') as lmv:\n",
    "    with h5py.File(os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "        for prob in imv.keys():\n",
    "            img = imv[prob][:]\n",
    "            # img = np.pad(img, padding, \"constant\", constant_values=0)\n",
    "            img = np.pad(img, padding, \"constant\", constant_values=np.min(img))\n",
    "\n",
    "            labs = lmv[prob][:]\n",
    "            neg_ones = labs < 0\n",
    "            labs = labs + padding[:, 0] \n",
    "            labs[neg_ones] = -1 # Retain -1s\n",
    "\n",
    "            oimv.create_dataset(prob, data = img)\n",
    "            olmv.create_dataset(prob, data = labs)\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(in_dir, \"fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_labels.h5\"), 'r') as lfx:\n",
    "    with h5py.File(os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "        for prob in ifx.keys():\n",
    "            img = ifx[prob][:]\n",
    "            # img = np.pad(img, padding, \"constant\", constant_values=0)\n",
    "            img = np.pad(img, padding, \"constant\", constant_values=np.min(img))\n",
    "\n",
    "            labs = lfx[prob][:]\n",
    "            neg_ones = labs < 0\n",
    "            labs = labs + padding[:, 0] \n",
    "            labs[neg_ones] = -1 # Retain -1s\n",
    "            \n",
    "            oifx.create_dataset(prob, data = img)\n",
    "            olfx.create_dataset(prob, data = labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceiling\n",
    "#### log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation == \"sqrt\":\n",
    "\n",
    "    in_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "    out_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"log_scaled\")\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"moving_labels.h5\"), 'r') as lmv:\n",
    "        with h5py.File(os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "            for prob in imv.keys():\n",
    "                img = imv[prob][:]\n",
    "                labs = lmv[prob][:]\n",
    "                \n",
    "                img = np.log2(img + 1, dtype=np.float32)\n",
    "                # img = np.log(img + 1, dtype=np.float32)\n",
    "\n",
    "                oimv.create_dataset(prob, data = img)\n",
    "                olmv.create_dataset(prob, data = labs)\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_labels.h5\"), 'r') as lfx:\n",
    "        with h5py.File(os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "            for prob in ifx.keys():\n",
    "                img = ifx[prob][:]\n",
    "                labs = lfx[prob][:]\n",
    "                \n",
    "                img = np.log2(img + 1, dtype=np.float32)\n",
    "                # img = np.log(img + 1, dtype=np.float32)\n",
    "                \n",
    "                oifx.create_dataset(prob, data = img)\n",
    "                olfx.create_dataset(prob, data = labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation == \"log\":\n",
    "\n",
    "    in_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "    out_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"sqrt_scaled\")\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"moving_images.h5\"), 'r') as imv,  h5py.File(os.path.join(in_dir, \"moving_labels.h5\"), 'r') as lmv:\n",
    "        with h5py.File(os.path.join(out_dir, \"moving_images.h5\"), 'w-') as oimv,  h5py.File(os.path.join(out_dir, \"moving_labels.h5\"), 'w-') as olmv:\n",
    "            for prob in imv.keys():\n",
    "                img = imv[prob][:]\n",
    "                labs = lmv[prob][:]\n",
    "                \n",
    "                img = np.sqrt(img, dtype=np.float32)\n",
    "\n",
    "                oimv.create_dataset(prob, data = img)\n",
    "                olmv.create_dataset(prob, data = labs)\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(os.path.join(in_dir, \"fixed_images.h5\"), 'r') as ifx,  h5py.File(os.path.join(in_dir, \"fixed_labels.h5\"), 'r') as lfx:\n",
    "        with h5py.File(os.path.join(out_dir, \"fixed_images.h5\"), 'w-') as oifx,  h5py.File(os.path.join(out_dir, \"fixed_labels.h5\"), 'w-') as olfx:\n",
    "            for prob in ifx.keys():\n",
    "                img = ifx[prob][:]\n",
    "                labs = lfx[prob][:]\n",
    "                \n",
    "                img = np.sqrt(img, dtype=np.float32)\n",
    "                \n",
    "                oifx.create_dataset(prob, data = img)\n",
    "                olfx.create_dataset(prob, data = labs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_1087to0_0', '0_1197to0_0', '0_1226to0_0', '0_1299to0_0', '0_1372to0_0', '0_1399to0_0', '0_142to0_0', '0_1451to0_0', '0_1575to0_0', '0_1579to0_0', '0_1599to0_0', '0_1613to0_0', '0_1624to0_0', '0_1630to0_0', '0_1649to0_0', '0_1893to0_0', '0_2003to0_0', '0_2177to0_0', '0_2224to0_0', '0_2238to0_0', '0_224to0_0', '0_2413to0_0', '0_2475to0_0', '0_2676to0_0', '0_2703to0_0', '0_2795to0_0', '0_2963to0_0', '0_2984to0_0', '0_3465to0_0', '0_3487to0_0', '0_3660to0_0', '0_3696to0_0', '0_3738to0_0', '0_3879to0_0', '0_4013to0_0', '0_4077to0_0', '0_407to0_0', '0_4255to0_0', '0_4493to0_0', '0_4575to0_0', '0_4599to0_0', '0_474to0_0', '0_4852to0_0', '0_4908to0_0', '0_4988to0_0', '0_4991to0_0', '0_5055to0_0', '0_5226to0_0', '0_5342to0_0', '0_5481to0_0', '0_5487to0_0', '0_5505to0_0', '0_5541to0_0', '0_5558to0_0', '0_5677to0_0', '0_5757to0_0', '0_5891to0_0', '0_591to0_0', '0_5933to0_0', '0_5985to0_0', '0_6012to0_0', '0_6122to0_0', '0_6123to0_0', '0_6215to0_0', '0_653to0_0', '0_6716to0_0', '0_680to0_0', '0_6909to0_0', '0_6911to0_0', '0_6968to0_0', '0_7376to0_0', '0_7377to0_0', '0_740to0_0', '0_7559to0_0', '0_7564to0_0', '0_7722to0_0', '0_7746to0_0', '0_7840to0_0', '0_7864to0_0', '0_8178to0_0', '0_8185to0_0', '0_8279to0_0', '0_8375to0_0', '0_8411to0_0', '0_8516to0_0', '0_8583to0_0', '0_8645to0_0', '0_8704to0_0', '0_8800to0_0', '0_8937to0_0', '0_9122to0_0', '0_913to0_0', '0_9172to0_0', '0_9266to0_0', '0_9402to0_0', '0_9583to0_0', '0_9607to0_0', '0_962to0_0', '0_9632to0_0', '0_9937to0_0', '1_1006to1_0', '1_1044to1_0', '1_1045to1_0', '1_1079to1_0', '1_1114to1_0', '1_1144to1_0', '1_1245to1_0', '1_1340to1_0', '1_1389to1_0', '1_1400to1_0', '1_1456to1_0', '1_1485to1_0', '1_1570to1_0', '1_1680to1_0', '1_173to1_0', '1_1871to1_0', '1_2066to1_0', '1_2097to1_0', '1_2224to1_0', '1_2300to1_0', '1_2344to1_0', '1_2398to1_0', '1_2442to1_0', '1_2534to1_0', '1_2686to1_0', '1_2701to1_0', '1_2766to1_0', '1_3062to1_0', '1_3177to1_0', '1_3241to1_0', '1_3314to1_0', '1_3444to1_0', '1_3835to1_0', '1_3857to1_0', '1_3933to1_0', '1_4006to1_0', '1_4055to1_0', '1_4165to1_0', '1_4546to1_0', '1_4599to1_0', '1_4784to1_0', '1_4958to1_0', '1_5055to1_0', '1_5073to1_0', '1_5233to1_0', '1_5390to1_0', '1_5426to1_0', '1_5435to1_0', '1_5515to1_0', '1_5594to1_0', '1_5672to1_0', '1_5727to1_0', '1_5755to1_0', '1_5773to1_0', '1_5831to1_0', '1_5862to1_0', '1_5903to1_0', '1_5929to1_0', '1_5961to1_0', '1_638to1_0', '1_6549to1_0', '1_6573to1_0', '1_6605to1_0', '1_6736to1_0', '1_7277to1_0', '1_7291to1_0', '1_7312to1_0', '1_7403to1_0', '1_7453to1_0', '1_7556to1_0', '1_7658to1_0', '1_7728to1_0', '1_7783to1_0', '1_7819to1_0', '1_805to1_0', '1_8076to1_0', '1_812to1_0', '1_8141to1_0', '1_8199to1_0', '1_8280to1_0', '1_839to1_0', '1_8417to1_0', '1_8603to1_0', '1_8732to1_0', '1_8750to1_0', '1_8763to1_0', '1_8850to1_0', '1_8901to1_0', '1_8942to1_0', '1_9104to1_0', '1_9112to1_0', '1_9146to1_0', '1_9229to1_0', '1_9404to1_0', '1_9497to1_0', '1_9528to1_0', '1_953to1_0', '1_9660to1_0', '1_9662to1_0', '1_9816to1_0', '2_1056to2_0', '2_1247to2_0', '2_1372to2_0', '2_1377to2_0', '2_1458to2_0', '2_1472to2_0', '2_1495to2_0', '2_1502to2_0', '2_1522to2_0', '2_153to2_0', '2_1570to2_0', '2_1574to2_0', '2_1630to2_0', '2_1655to2_0', '2_1683to2_0', '2_1744to2_0', '2_174to2_0', '2_1792to2_0', '2_1866to2_0', '2_1964to2_0', '2_1997to2_0', '2_1to2_0', '2_204to2_0', '2_2067to2_0', '2_211to2_0', '2_2136to2_0', '2_2148to2_0', '2_2154to2_0', '2_2180to2_0', '2_2259to2_0', '2_232to2_0', '2_2503to2_0', '2_2635to2_0', '2_2690to2_0', '2_2757to2_0', '2_2832to2_0', '2_2933to2_0', '2_2980to2_0', '2_2986to2_0', '2_3060to2_0', '2_3094to2_0', '2_3144to2_0', '2_3161to2_0', '2_3163to2_0', '2_3164to2_0', '2_3399to2_0', '2_339to2_0', '2_3471to2_0', '2_3516to2_0', '2_3525to2_0', '2_3531to2_0', '2_3535to2_0', '2_3567to2_0', '2_3668to2_0', '2_3719to2_0', '2_375to2_0', '2_3789to2_0', '2_3836to2_0', '2_4163to2_0', '2_4169to2_0', '2_4224to2_0', '2_4266to2_0', '2_4325to2_0', '2_4409to2_0', '2_4569to2_0', '2_4766to2_0', '2_476to2_0', '2_4799to2_0', '2_4824to2_0', '2_4837to2_0', '2_4900to2_0', '2_5170to2_0', '2_5292to2_0', '2_5331to2_0', '2_5420to2_0', '2_5428to2_0', '2_544to2_0', '2_5490to2_0', '2_549to2_0', '2_5505to2_0', '2_5522to2_0', '2_5540to2_0', '2_559to2_0', '2_5680to2_0', '2_5711to2_0', '2_5803to2_0', '2_5841to2_0', '2_5852to2_0', '2_5891to2_0', '2_5953to2_0', '2_730to2_0', '2_741to2_0', '2_771to2_0', '2_838to2_0', '2_846to2_0', '2_860to2_0', '2_934to2_0', '2_94to2_0', '2_952to2_0', '2_957to2_0']\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(new_fixed_h5, 'r') as f:\n",
    "    probs = list(f.keys())\n",
    "    print(probs)\n",
    "    print(len(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into training and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_2442to1_0', '1_8141to1_0', '2_3471to2_0', '2_4169to2_0', '1_8850to1_0', '0_9583to0_0', '1_953to1_0', '1_1044to1_0', '2_5891to2_0', '2_94to2_0']\n"
     ]
    }
   ],
   "source": [
    "val_probs = random.sample(probs, 10)\n",
    "print(val_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_probs = [\n",
    "#             \"3563to15011\",\n",
    "#             \"5004to15011\",\n",
    "#             \"6739to15011\",\n",
    "#             \"8110to15011\",\n",
    "#             \"985to15011\"\n",
    "#         ]\n",
    "# val_probs = ['4175to0000', '8140to0000', '5347to0000', '5683to0000', '5127to0000', '4480to0000', '5002to0000', '5417to0000', '8261to0000', '4215to0000', '5242to0000', '5753to0000', '5067to0000', '4455to0000', '4981to0000', '5082to0000', '5868to0000', '5843to0000', '5437to0000', '5948to0000', '8014to0000', '4871to0000', '5152to0000', '4691to0000', '8351to0000', '5748to0000', '5643to0000', '2841to0000', '4200to0000', '4035to0000', '8289to0000', '5728to0000', '5037to0000', '4606to0000', '4821to0000', '5563to0000', '5693to0000', '8002to0000', '5057to0000', '8273to0000', '5462to0000', '8203to0000', '4831to0000', '8297to0000', '4956to0000', '4450to0000', '5302to0000', '8022to0000', '4656to0000', '3999to0000', '5593to0000', '5653to0000', '4936to0000', '4561to0000', '5192to0000', '4060to0000', '5257to0000', '8367to0000', '5117to0000', '8257to0000', '4811to0000', '7683to0000', '5327to0000', '5402to0000', '8196to0000', '2105to0000', '4651to0000', '4566to0000', '4766to0000']\n",
    "# val_probs = ['2984to0000', '3838to0000', '3315to0000', '0765to0000', '2572to0000', '3235to0000', '5215to0000', '0738to0000', '2683to0000', '5295to0000', '1662to0000', '4472to0000', '2773to0000', '5727to0000', '3305to0000', '1389to0000', '5396to0000', '4010to0000', '4883to0000', '4190to0000', '1688to0000', '1125to0000', '1468to0000', '2833to0000', '3657to0000', '1512to0000', '0712to0000', '1037to0000', '0413to0000', '2210to0000', '3567to0000', '0923to0000', '2522to0000', '3546to0000', '0263to0000', '5125to0000', '0395to0000', '0536to0000', '3205to0000', '5999to0000', '2552to0000', '1116to0000', '1407to0000', '0571to0000', '5506to0000', '5235to0000', '0114to0000', '3737to0000', '3587to0000']\n",
    "\n",
    "if augmentation == None:\n",
    "    base_dir = os.path.join(base_folder, \"cropped\", \"padded\")\n",
    "elif augmentation == \"sqrt\":\n",
    "    base_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"sqrt_scaled\")    \n",
    "elif augmentation == \"log\":\n",
    "    base_dir = os.path.join(base_folder, \"cropped\", \"padded\", \"log_scaled\") \n",
    "else:\n",
    "    raise ValueError(f\"Unknown augmentation {augmentation}\")\n",
    "\n",
    "os.mkdir(os.path.join(base_dir, \"train\"))\n",
    "os.mkdir(os.path.join(base_dir, \"val\"))\n",
    "\n",
    "old_img_fixed_h5 = f\"{base_dir}/fixed_images.h5\"\n",
    "train_img_fixed_h5 = f\"{base_dir}/train/fixed_images.h5\"\n",
    "val_img_fixed_h5 = f\"{base_dir}/val/fixed_images.h5\"\n",
    "\n",
    "old_label_fixed_h5 = f\"{base_dir}/fixed_labels.h5\"\n",
    "train_label_fixed_h5 = f\"{base_dir}/train/fixed_labels.h5\"\n",
    "val_label_fixed_h5 = f\"{base_dir}/val/fixed_labels.h5\"\n",
    "\n",
    "old_img_moving_h5 = f\"{base_dir}/moving_images.h5\"\n",
    "train_img_moving_h5 = f\"{base_dir}/train/moving_images.h5\"\n",
    "val_img_moving_h5 = f\"{base_dir}/val/moving_images.h5\"\n",
    "\n",
    "old_label_moving_h5 = f\"{base_dir}/moving_labels.h5\"\n",
    "train_label_moving_h5 = f\"{base_dir}/train/moving_labels.h5\"\n",
    "val_label_moving_h5 = f\"{base_dir}/val/moving_labels.h5\"\n",
    "\n",
    "\n",
    "with h5py.File(train_img_fixed_h5, 'w-') as tif,  h5py.File(val_img_fixed_h5, 'w-') as vif:\n",
    "    with h5py.File(train_label_fixed_h5, 'w-') as tlf,  h5py.File(val_label_fixed_h5, 'w-') as vlf:\n",
    "        with h5py.File(old_img_fixed_h5, 'r') as oif:\n",
    "            with h5py.File(old_label_fixed_h5, 'r') as olf:\n",
    "                for prob in oif.keys():\n",
    "                    img = oif[prob][:]\n",
    "                    labs = olf[prob][:]\n",
    "                    if prob in val_probs:\n",
    "                        vif.create_dataset(prob, data = img)\n",
    "                        vlf.create_dataset(prob, data = labs)\n",
    "                    else:\n",
    "                        tif.create_dataset(prob, data = img)\n",
    "                        tlf.create_dataset(prob, data = labs)\n",
    "        \n",
    "\n",
    "with h5py.File(train_img_moving_h5, 'w-') as tim,  h5py.File(val_img_moving_h5, 'w-') as vim:\n",
    "    with h5py.File(train_label_moving_h5, 'w-') as tlm,  h5py.File(val_label_moving_h5, 'w-') as vlm:\n",
    "        with h5py.File(old_img_moving_h5, 'r') as oim:\n",
    "            with h5py.File(old_label_moving_h5, 'r') as olm:\n",
    "                for prob in oim.keys():\n",
    "                    img = oim[prob][:]\n",
    "                    labs = olm[prob][:]\n",
    "                    if prob in val_probs:\n",
    "                        vim.create_dataset(prob, data = img)\n",
    "                        vlm.create_dataset(prob, data = labs)\n",
    "                    else:\n",
    "                        tim.create_dataset(prob, data = img)\n",
    "                        tlm.create_dataset(prob, data = labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_img_moving_h5 = f\"{base_dir}/moving_images.h5\"\n",
    "\n",
    "train_roi_moving_h5 = f\"{base_dir}/train/moving_rois.h5\"\n",
    "val_roi_moving_h5 = f\"{base_dir}/val/moving_rois.h5\"\n",
    "train_roi_fixed_h5 = f\"{base_dir}/train/fixed_rois.h5\"\n",
    "val_roi_fixed_h5 = f\"{base_dir}/val/fixed_rois.h5\"\n",
    "\n",
    "\n",
    "with h5py.File(val_roi_fixed_h5, 'w-') as vrf,  h5py.File(val_roi_moving_h5, 'w-') as vrm:\n",
    "# with h5py.File(train_roi_fixed_h5, 'w-') as trf,  h5py.File(train_roi_moving_h5, 'w-') as trm:\n",
    "    with h5py.File(old_img_moving_h5, 'r') as oim:\n",
    "        for prob in oim.keys():\n",
    "            blank = np.zeros_like(oim[prob][:])\n",
    "            vrf.create_dataset(prob, data = blank)\n",
    "            vrm.create_dataset(prob, data = blank)\n",
    "            # trf.create_dataset(prob, data = blank)\n",
    "            # trm.create_dataset(prob, data = blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
